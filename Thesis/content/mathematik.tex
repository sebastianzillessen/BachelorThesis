%Die Angabe des schlauen Spruchs auf diesem Wege funtioniert nur,
%wenn keine Änderung des Kapitels mittels den in preambel/chapterheads.tex
%vorgeschlagenen Möglichkeiten durchgeführt wurde.
%\setchapterpreamble[u]{%
%\dictum[Albert Einstein]{Insofern sich die Sätze der Mathematik auf die Wirklichkeit beziehen, sind sie nicht sicher, und insofern sie sicher sind, beziehen sie sich nicht auf die Wirklichkeit. Mathematische Theorien über die Wirklichkeit sind immer ungesichert - wenn sie gesichert sind, handelt es sich nicht um die Wirklichkeit.}
%}
\chapter{Mathematische Ausarbeitung}
\label{chap:maths}
Zur Lösung des \gls{LGS} aus \autoref{eq:energy:weights} soll in dieser Arbeit ein anderes Verfahren verwendet werden. Hierbei wird das Lösen nach $\b g$ und $E_i$ in zwei Probleme zerlegt und durch ein alternierendes Lösungsverfahren ersetzt. Der Vorteil dieses Ansatzes ist, dass die gesamten Bildinformationen aus der Belichtungsserie verwendet werden können. Dies ist möglich, da die entstehenden Gleichungssysteme sehr dünn besetzt sind und effizient gelöst werden können. Die Struktur des alternierenden Vorgehens ist im \autoref{alg:alternierend:basic} beschrieben.

\begin{Algorithmus} %Die Umgebung nur benutzen, wenn man den Algorithmus ähnlich wie Graphiken von TeX platzieren lassen möchte
\caption{Alternierendes Lösen nach $g(k)$ und $E_i$}
\label{alg:alternierend:basic}
\begin{algorithmic}
\Function{SolveHDR}{$Z_{ij}$, $\ln \Delta t_j$, $N$, $P$}
	\State $g \gets initG()$
	\While{$g$ changes} 
		\State $\b F \gets solveF(\b F, \b g, Z_{ij}, \ln \Delta t_j, N, P)$ \Comment{$F_i = \ln E_i$}
		\State $\b g \gets solveG(\b F, \b g, Z_{ij}, \ln \Delta t_j, N, P)$
	\EndWhile
	\State \Return [$\b g$, $\b F$]
\EndFunction
\end{algorithmic}
\end{Algorithmus}

Der Vorteil dieses Vorgehens ist, dass neben der Schätzung der Kamera-Antwortkurve auch gleichzeitig die \gls{Radiance Map} des \gls{HDR}-Bildes mit berechnet wird. Dadurch spart man sich die anschließende Umrechnung der Bildpunkte mittels der Funktion $\b g$ und ist darüber hinaus auch in der Lage Forderungen an $\b E$ zu stellen.

In den nachfolgenden Abschnitten werden häufig Approximationen für die erste und zweite Ableitung verwendet. Dass es sich hierbei deswegen in der Regel um keine exakte Gleichheit ($=$) handelt, sondern vielmehr um eine Annäherung ($\approx$) sei hier erwähnt. Es wird im Nachfolgenden aus Gründen der Lesbarkeit darauf verzichtet dies kenntlich zu machen.


% --------------------------------------------------------------------------------------------------------------
\section{Optimierungsansatz}
\label{sec:ansatz}
Die Gleichung aus \autoref{eq:energy:weights} dient als Grundlage für den Optimierungsansatz des gesamten Verfahrens. Da dieses Energiefunktional minimiert werden soll, sind partielle Ableitungen nach $\b g(k)$ bzw. $\ln E_i = F_i$ notwendig. Die vorkommenden Ableitungen zweiter Ordnung werden mittels der zentralen Differenz $\b g''(k) = \b g(k-1) - 2\b g(k) +\b g(k+1)$ diskretisiert (siehe \autoref{eq:energy:diskret}).

\begin{align}
\Omega &= \sum \limits_{i=1}^{N} \sum \limits_{j=1}^{P}w^2(Z_{ij})\cdot[\b g(Z_{ij}) - \ln E_i - \ln \Delta t_j]^2 + \lambda  \sum \limits_{z=Z_{min}+1}^{Z_{max}-1} [w(z) \cdot \b g''(z)]^2\\
\label{eq:energy:diskret}
\begin{split}
 &= \underbrace{\sum \limits_{i=1}^{N} \sum \limits_{j=1}^{P}w^2(Z_{ij})\cdot[\b g(Z_{ij}) - F_i - \ln \Delta t_j]^2}_{\Phi} \\
 &+ \lambda \underbrace{ \sum \limits_{z=Z_{min}+1}^{Z_{max}-1} w^2(z) \cdot \overbrace{
 	[\b g(z-1)-2\b g(z)+\b g(z+1)]^2
 }^{
 	\text{Diskretisierung von }g''(k)
 }
 }_{\Theta}
 \end{split}\\
 \Omega &= \Phi + \lambda \Theta
\end{align}

In den folgenden Herleitungen taucht häufig der Faktor $2$ auf. Dieser entsteht durch das Ableiten der quadratischen Bestrafungsfunktionen. Er taucht in der Regel in allen Summanden von $\partial \Omega$ auf und kann deswegen gekürzt werden. In besonderen Fällen (wie z.B. der Erweiterung um robuste Bestrafungsterme, siehe \autoref{sec:robustheit}) ist das nicht der Fall. Dann werden diese Faktoren separat behandelt.

% --------------------------------------------------------------------------------------------------------------
\subsection{Gleichungssystem für $\b g$}
Um nun das \gls{LGS} zur Lösung nach $g$ aufzustellen, muss $\Omega$ zunächst partiell nach $\b g(k) \; \forall k \in [0,255]$ abgeleitet werden (siehe \autoref{eq:energy:partial}).

\begin{align}
\label{eq:energy:partial}
\frac{\partial \Omega}{\partial \b g(k)} & = \frac{\partial \Phi}{\partial \b g(k)} +\frac{\partial \Theta}{\partial \b g(k)}\\
\frac{\partial \Phi}{\partial \b g(k)} &= 2\cdot w^2(k) \cdot \sum \limits_{i=1}^{N} \sum \limits_{j=1}^{P} \cdot[\b g(k) - \ln E_i - \ln \Delta t_j]\cdot \delta_{Z_{ij}=k} \qquad \delta_{z=k} = \begin{cases}
    1 \mbox{ wenn } z = k\\
    0 \mbox{ sonst}
\end{cases}\\
&=2\cdot w^2(k) \cdot \b g(k) \sum \limits_{i=1}^{N} \sum \limits_{j=1}^{P}\delta_{Z{ij}=k} - 2w^2(k)\cdot\sum \limits_{i=1}^{N} \sum \limits_{j=1}^{P}\delta_{Z_{ij}=k}(\ln E_i - \ln \Delta t_j)
\end{align}

Da $\Omega$ minimiert werden soll, gilt $\Omega' \overset{!}{=} 0 \Rightarrow (\Theta' \overset{!}{=} 0 \wedge \Phi' \overset{!}{=} 0)$. Daraus entsteht das lineare Gleichungssystem für den Datenterm in \autoref{eq:matrix:data}. Die Koeffizienten der Matrix können aus den einzelnen partiellen Ableitungen gewonnen werden.
\begin{align}
\frac{\partial \Phi}{\partial \b g(k)} 
    \overset{!}{=} 0 & = 
    2 w^2(k) [\b g(k) \sum_{i=1}^{N} \sum_{j=1}^{P}\delta_{Z_{ij}=k} - 
    \sum_{i=1}^{N} \sum_{j=1}^{P}(\ln E_i - \ln \Delta t_j)\delta_{Z_{ij}=k}]\\
    \underbrace{
        w^2(k) \sum_{i=1}^{N} \sum_{j=1}^{P}(\delta_{Z_{ij}=k})
    }_{\mbox{Matrixeintrag }a_k} 
    \cdot \b g(k) &= 
    \underbrace{
        w^2(k) \sum_{i=1}^{N} \sum_{j=1}^{P}(\ln E_i - \ln \Delta t_j)\delta_{Z_{ij}=k}
    }_{\mbox{Vektoreintrag }b_k} \\ 
\begin{pmatrix}
\ddots & 0& 0\\
0 & a_k & 0\\
0 & 0 & \ddots
\end{pmatrix}
 \cdot \begin{pmatrix}
 \vdots\\
 \b g(k)\\
 \vdots
 \end{pmatrix} &= \label{eq:matrix:data}
\begin{pmatrix}
 \vdots\\
 b_k\\
 \vdots
 \end{pmatrix} 
\end{align}


Anschließend betrachten wir den Glattheitsterm $\Theta$. Auch dieser muss partiell nach $\b g(k)$ abgeleitet werden. Aus Gründen der Vereinfachung wurden in den folgenden Berechnungen $Z_{min} = 0$ und $Z_{max} = 255$ angenommen. In der \autoref{eq:energy:diskret} wurde der Gewichtungsfaktor für den Glattheitsterm $\lambda$ absichtlich nicht in $\Theta$ integriert, da dieser bei der Herleitung keine Rolle spielt. Der Faktor $\lambda$ wird am Ende wieder hinzugefügt. Bei der partiellen Ableitung des Glattheitsterms muss hier besonders auf die Randbedingungen geachtet werden, dort verhält sich die partielle Ableitung anders:

\begin{align}
\label{eq:partials:smoothness:0}
\frac{\partial \Theta}{\partial \b g(0)} =& w^2(1)\cdot \b g(0) -2w^2(1)\cdot \b g(1) + w^2(1)\cdot \b g(2) = 0\\
\label{eq:partials:smoothness:1}
\frac{\partial \Theta}{\partial \b g(1)} = &-2w^2(1)\cdot \b g(0) \nonumber \\
        &+[4w^2(1)+w^2(2)]\cdot \b g(1) \nonumber \\
        &- 2 [w^2(1)+w^2(2)]\cdot \b g(2) \nonumber \\
        &+ w^2(2)\cdot \b g(3) \nonumber \\
        &= 0\\
\label{eq:partials:smoothness:k}
\frac{\partial \Theta}{\partial \b g(k)} =& 
        w^2(k-1)\cdot \b g(k-2)\nonumber\\
        &- 2[w^2(k-1)+2w^2(k)]\cdot \b g(k-1) \nonumber\\
        &+ [w^2(k-1)+4w^2(k)+w^2(k+1)]\cdot \b g(k)\nonumber\\ 
        &- 2[w^2(k)+w^2(k+1)] \cdot \b g(k+1) \nonumber\\
        &+ w^2(k+1)\cdot \b g(k+1) \nonumber\\
        &=0 \qquad \forall k \in [2,253]\\
\label{eq:partials:smoothness:254}
\frac{\partial \Theta}{\partial \b g(254)} =& w^2(253)\cdot \b g(252) \nonumber \\
        & -2 (w^2(253)+w^2(254))\cdot \b g(253) \nonumber \\
        & +(w^2(253)+4 w^2(254)) \cdot \b g(254)\nonumber \\
        & -2 w^2(254)\cdot \b g(255)\nonumber \\ 
        &= 0\\
\label{eq:partials:smoothness:255}
\frac{\partial \Theta}{\partial \b g(255)} &= 
    w^2(254)\cdot \b g(253) 
    - 2 w^2(254)\cdot \b g(254) 
    + w^2(254)\cdot \b g(255)= 0
\end{align}


Aus diesen Gleichungen kann nun das lineare Gleichungssystem (siehe \autoref{eq:matrix:smoothness}) aufgestellt werden. Die  Koeffizienten der Matrix gehen aus obigen Gleichungen hervor (z.B. $d_{0,0} = w^2(1), \; d_{1,-1} = -2w^2(1), \dots$). Der Faktor $\lambda$ wurde hier wieder mit integriert (siehe \autoref{eq:energy:diskret}).

\begin{align}
\label{eq:matrix:smoothness}
\lambda
\underbrace{
\begin{pmatrix}
d_{0,0} & d_{0,1} & d_{0,2} & 0 &\cdots\\
d_{1,-1} & d_{1,0} & d_{1,1} & d_{1,2} & 0 &\cdots\\
&  & \ddots &  \\
\cdots  & d_{k,-2} & d_{k,-1} & d_{k,0}&d_{k,1} &d_{k,2} & \cdots\\
&&&&\ddots&&&\\
&&&d_{254,-2}&d_{254,-1}&d_{254,0}&d_{254,1}\\
&&&&d_{255,-2}&d_{255,-1}&d_{255,0}\\
\end{pmatrix}}_{\mbox{Matrix } D_4}
\cdot
\begin{pmatrix}
\vdots\\
\b g(k)\\
\vdots
\end{pmatrix}
= 
\begin{pmatrix}
\vdots\\
0\\
\vdots
\end{pmatrix}
\end{align}


Gemeinsam ergeben die Gleichungssysteme für den Datenterm (siehe \autoref{eq:matrix:data}) und den Glattheitsterm das endgültige Gleichungssystem für $\b g$, welches ebenfalls in Matrix-Notation aufgestellt werden kann.
 
\begin{align}
\label{eq:g:lgs}
\underbrace{[A+\lambda D_4]}_{\mbox{Matrix M}}\cdot \b g = b
\end{align}

Zu beachten ist, dass $M$ eine \gls{Pentadiagonal-Matrix} ist. Dies kann beim Lösen des \gls{LGS} genutzt werden, indem eine spezialisierte Variante der LU-Zerlegung verwendet wird (siehe \autoref{sec:maths:lu}).

Außerdem führt die Zerlegung des Problems in das separierte Lösen nach $\b g$ und $\b E$ dazu, dass die ursprünglich erwähnte Eigenschaft der unendlichen Anzahl von Lösungen (siehe \autoref{sec:eindeutigkeit}) nicht mehr besteht. Dies ist erst während der Implementierung des Ansatzes aufgefallen und liegt daran, dass die beiden Schritte des Verfahrens separat und alternierend ausgeführt werden und immer eine konkrete Näherungslösung der jeweils anderen Unbekannten vorliegen muss.
Um diese Problematik zu umgehen, wird deshalb nach der Berechnung von $\b g$ die Kurve immer so verschoben, dass $\b g(Z_{mid}) = 0$ gilt. 

\begin{align}
\tilde{\b g}(k) &= \b g(k)-\b g(Z_{mid})\, \forall k \in [0,255]
\end{align}

Damit ist sichergestellt, dass alle Antwortkurven, die durch das Verfahren berechnet werden, vergleichbar und eindeutig sind.


% --------------------------------------------------------------------------------------------------------------

\subsection{Lösen von $E$}
Für $E$ muss in der Fassung des Algorithmus ohne räumlichen Glattheitsterm (siehe \autoref{sec:raeumlich}) kein lineares Gleichungssystem  gelöst werden. Hier können die Werte direkt berechnet werden, da $\b g$ bekannt ist. Dazu wird das Energiefunktional aus \autoref{eq:energy:diskret} nach $\ln E_i = F_i$ partiell abgeleitet und umgeformt.    

\begin{align}
\label{eq:fi:basic}
    \ln E_i = F_i =& \frac{\sum \limits_{j=0}^{P-1} (\b g(Z_{ij})-\ln \Delta t_j)\cdot w(Z_{ij})}{\sum \limits_{j=0}^{P-1} w(Z_{ij})}
\end{align}


% ----------------------------------------------------------------------------------

\section{Erweiterung um Monotonie-Eigenschaft}
\label{sec:monotonie}

Bereits im Ansatz von Debevec und Malik wird für die Funktion $f$ angenommen, dass sie monoton und damit invertierbar ist. Für die Funktion $\b g$ wird diese Forderung jedoch nicht weiter aufgenommen. Aus physikalischer und mathematischer Sicht muss $\b g$ jedoch ebenfalls monoton sein, da ansonsten die Annahme $f$ sei monoton nicht gelten würde. Deshalb wurde eine erste Erweiterung des Algorithmus mit einer Forderung an die Monotonie von $\b g$ realisiert. Dies lässt sich über das Energiefunktional $\Omega$ als weiteren Bestrafungsterm $\Gamma$ realisieren.

\begin{align}
\tilde{\Omega} &= \Omega + \underbrace{\mu \sum \limits_{z=1}^{255} w^2(z) [(\phi_{\b g'<0}(z)\cdot \b g'(z)]^2}_{\mbox{Monotonie-Forderung } \Gamma} = \Omega + \Gamma\\
\phi_{\b g'<0}(z) &= 
    \begin{cases} 
        1, \; \mbox{falls } \b g'(z) < 0\\ 
        0 \; \mbox{sonst}
    \end{cases}
\end{align}

Auch hier wird wieder der Least-Square-Ansatz (quadratische Bestrafungsfunktion) verwendet, welcher auch schon im Standard-Verfahren zum Einsatz kommt (siehe \autoref{eq:energy:weights}). Der Operator $\phi_{\b g'<0} (z)$ sorgt dafür, dass nur die Werte von $\b g$ bestraft werden, die nicht monoton steigend sind. Da $\b g'(z)$ bei der Berechnung von $\b g$ jedoch nicht bekannt ist, wird für diese Einschaltfunktion die Instanz $\b g$ aus der vorherigen Iteration verwendet. Durch die Diskretisierung mit $\b g'(z) = \b g(z)-\b g(z-1)$ erhält man damit:

\begin{align}
\Gamma = & \mu \sum_{z=1}^{255} w^2(z) \cdot \phi^2_{\b g'<0}(z) \cdot (\b g(z)- \b g(z-1))^2 \qquad \mbox{ (Disketisierung)}\\
\label{eq:mon:diskret}
\frac{\partial \Gamma}{\partial \b g(0)} =& -2\mu w^2(1) \cdot \phi^2_{\b g'<0}(1)\cdot(\b g(1)-\b g(0))\\\frac{\partial \Gamma}{\partial \b g(k)} 
        =& 2\mu w^2(k) \cdot \phi^2_{\b g'<0}(k)\cdot(\b g(k)-\b g(k-1)) \nonumber \\
         &- 2 \mu w^2(k+1) \cdot \phi^2_{\b g'<0}(k+1)\cdot(\b g(k+1)-\b g(k))
        \; , \; \forall k \in [1,254]\\
\frac{\partial \Gamma}{\partial \b g(255)} =& -2\mu w^2(255) \cdot \phi^2_{\b g'<0}(255)\cdot(\b g(255)-\b g(254))
\end{align}
Aus dieser Herleitung lässt sich nun wieder eine Matrix mit folgender Stuktur erzeugen (hier wurde $\phi^2_{\b g'<0}(k) = \phi^2_{\b g'}(k)$ zur Kürzung verwendet):
\small
\begin{align}
2\mu 
\underbrace{\begin{pmatrix}
-w^2(1)\phi^2_{\b g'}(1) &w^2(1)\phi^2_{\b g'}(1) & 0 & \cdots\\
& \ddots\\
\cdots & -w^2(k)\phi^2_{\b g'}(k) & 
\begin{smallmatrix}
w^2(k)\phi^2_{\b g'}(k) \\+ w^2(k+1)\phi^2_{\b g'}(k+1)
\end{smallmatrix}
 & -w^2(k+1)\phi^2_{\b g'}(k+1) & \cdots \\
& & \ddots\\
& \cdots &  0 & w^2(255)\phi^2_{\b g'}(255) & - w^2(255)\phi^2_{\b g'}(255) \\
\end{pmatrix}}_{\mbox{Matrix } C}
\end{align}
\normalsize

Diese Berechnung kann auch über Matritzenmultiplikation erreicht werden. $D$ ist dabei eine Matrix, welche die erste Ableitung approximiert. $V$ ist eine Diagonal-Matrix mit $v_{i,i} = \phi_{\b g'<0}(i)$. $W$ ist die Diagonal-Matrix der Gewichte mit $w_{i,i} = w(i)$. Die damit entstehende \autoref{eq:monotonie:matrix} kann dann partiell zur \autoref{eq:monotonie:derivate} abgeleitet werden.

\begin{align}
\label{eq:monotonie:matrix}
\Gamma &= \mu  \cdot (W V  D \b g)^2 = \mu (\b g^T \cdot D^T V^T W^T W V D \cdot \b g)\\
\label{eq:monotonie:derivate}
\frac{\partial \Gamma}{\partial \b g} &= 2 \mu \underbrace{D^T V^T W^T W V D}_{\mbox{Matrix }C}\cdot \b g \overset{!}{=} 0
\end{align}


Der Parameter $\mu$ ist ähnlich wie $\lambda$ ein Gewichtungsfaktor für die Monotonie-Bedingung. Die \autoref{eq:monotonie:derivate} kann damit einfach zum Gleichungssystem aus \ref{eq:g:lgs} hinzugenommen. Daraus entsteht folgendes zu lösende Gleichungssystem:

\begin{align}
\label{eq:g:monotonie}
[M + \mu C] \cdot \b g = b
\end{align}

Zu beachten ist hier, dass die Matrix $C$ ebenfalls pentadiagonal ist und somit auch bei diesem LGS die besondere Eigenschaft bestehen bleibt, wodurch auch hier die \nameref{sec:maths:lu} (siehe \autoref{sec:maths:lu}) verwendet werden kann.


%------------------------------------------------------------------------------------------------------------------
\section{Räumlicher Glattheitsterm}
\label{sec:raeumlich}
Die Erweiterung um den räumlichen Glattheitsterm (also eine Forderung an $\b E$ sich im zweidimensionalen Bild möglichst glatt zu verhalten) ist eine sinnvolle Erweiterung, um die Berechnung der $\ln E_i$ noch weiter zu optimieren und das lokale Umfeld um einen Bildpunkt mit in die Berechnung einzubeziehen. Dazu fordern wir eine räumliche Glattheit, die (analog zum Glattheitsterm von $\b g$) mittels der ersten Ableitung ausgedrückt werden kann. Da wir uns nun jedoch im zweidimensionalen Bildbereich befinden, müssen die Ableitungen in $x$- und $y$-Richtung betrachtet werden. Der Vektor $\b E$ ist eine eindimensionale Darstellung des zweidimensionalen Bildbereiches ($n \times m$), wobei gilt: $i = x+y*n$ ($x \in [0,n], \; y \in [0,m]$). 
Hierbei müssen die Ränder entsprechend behandelt werden.

\begin{align}
\label{eq:raum:glattheit}
\tilde{\Omega} =& 
    \Phi + \Theta +
    \underbrace{
        \alpha \sum_{i\in A}
            (\overbrace{
                \ln E_i - \ln E_{i-1}
            }^{\mbox{Abltg. nach }x}
        )^2
        +\alpha \sum_{i=n}^{N-1}(
            \overbrace{
                \ln E_i - \ln E_{i-n}
            }^{\mbox{Abltg. nach }y}
        )^2
    }_{\mbox{Glattheitsterm }\Psi}\\
    \label{eq:raum:x}
    A=& \{ i \in [0,N-1]\} \setminus \{ i \cdot k | k \in \mathbb{N}_+^0 \}
\end{align}

Dies muss nun wieder nach $\ln E_i$ partiell abgeleitet werden um das Minimum des Energiefunktionals (siehe \autoref{eq:raum:glattheit}) zu bestimmen. Auch hier wurde $\ln E_i$ durch $F_i$ ersetzt. Zunächst werden die Randbedingungen vernachlässigt.
\begin{align}
\label{eq:raum:derivate}
\frac{\partial \Psi}{\partial F_i} =& 2\alpha[(F_i - F_{i-1}) - (F_{i+1} - F_i) + (F_i - F_{i-n})-(F_{i+n}- F_i)]\\
=&2\alpha[4 F_i-F_{i-n} - F_{i-1} - F_{i+1} - F_{i+n}]
\end{align}


Der Einfluss der benachbarten Bildpunkte (siehe \autoref{fig:raum:stencil}) erinnert an einen Hochpass-Filter (eng. highpass filter), der in der Bildverarbeitung dazu verwendet wird, verschwommene (engl. blurry) Bilder zu verbessern.

\begin{figure}[h]
  \begin{center}
    \begin{tabular}{c|c|c}
        \cline{2-2}
        & -1 & \\
        \hline
        \multicolumn{1}{|c|}{-1}
        & 4 & \multicolumn{1}{c|}{-1}\\
        \hline
        & -1 & \\
        \cline{2-2} 
    \end{tabular}
  \end{center}
\caption{Einfluss der umliegenden Bildpunkte $F_i$ beim aktiviertem räumlichen Glattheitsterm. Hier in 2D dargestellt}
\label{fig:raum:stencil}
\end{figure}

Um nun das gesamte Gleichungssystem für $F_i$ aufzustellen, müssen zunächst noch die Terme $\Theta$ und $\Phi$ und ihre partiellen Ableitungen betrachtet werden:
\begin{align}
\frac{\partial \Theta}{\partial F} =& 0\\
\Phi =&\sum_{i=0}^{N-1} \sum_{j=0}^{P-1} w^2(Z_{ij})[\b g(Z_{ij}) - F_i - \ln \Delta t_j]^2\\
\frac{\partial \Phi}{\partial F_k} =& 2 \sum_{j=0}^{P-1} w^2(Z_{kj})[\b g(Z_{kj}) -F_k-\ln \Delta t_j]\\
\label{eq:raum:phi}
=& 2 \sum_{j=0}^{P-1} w^2(Z_{kj})[\b g(Z_{kj}) -\ln \Delta t_j] - 2 \sum_{j=0}^{P-1} w^2(Z_{kj})F_k \overset{!}{=}0\\
\Rightarrow & \qquad 2 \underbrace{\sum_{j=0}^{P-1} w^2(Z_{kj})[\b g(Z_{kj}) -\ln \Delta t_j]}_{\mbox{Vektoreintrag }b_k} = 2 \underbrace{\sum_{j=0}^{P-1} w^2[Z_{kj})}_{\mbox{Matrixeintrag } H_{k,k}}F_k\\
\label{eq:raum:phi:matrix}
\Rightarrow & \qquad 2 \b b = 2 H \cdot \b F
\end{align}


Aus der obigen \autoref{eq:raum:derivate} (in der die Ränder noch nicht beachtet wurden) kann nun die Matrix für die Einbindung der räumlichen Glattheitsforderung erstellt werden (Struktur siehe \autoref{fig:raum:matrix}).

Die entstehende Matrix $R$ ist eine $N \times N$ Matrix, die in Blöcken der Größe $n \times m$ aufgeteilt werden kann. Ein solcher Block auf der Diagonalen der Matrix  steht jeweils für eine Reihe von Bildpunkten im Bild (siehe \autoref{fig:raum:matrix}). 

Aus den Gleichungen \ref{eq:raum:derivate} und \ref{eq:raum:phi} können nun die einzelnen Gleichungen für $\b F_k$ erstellt werden:

\begin{align}
 0 = 2 \sum_{j=0}^{P-1} w^2(Z_{kj})[\b g(Z_{kj}) -\ln \Delta t_j] - 2 \sum_{j=0}^{P-1} w^2(Z_{kj})F_k + 2\alpha [4 F_k-F_{k-n} - F_{k-1} - F_{k+1} - F_{k+n}]
\end{align}

Aus der Gleichung \ref{eq:raum:phi:matrix} und der Struktur der Matrix $R$ (siehe \autoref{fig:raum:matrix}) lässt sich damit das nachfolgende Gleichungssystem für die Berechnung von $\b F$ aufstellen. 

\begin{align}
2 H \cdot \b F + 2 \alpha R\cdot \b F =& 2 \b b \\
\label{eq:raum:res}
(H+\alpha R)\cdot \b F =& \b b
\end{align}

Das Gleichungssystem aus \ref{eq:raum:res} ist symmetrisch, quadratisch und positiv semi definit. Aufgrund dieser Eigenschaften kann beim Lösen des \gls{LGS} das schnell konvergierende SOR-Verfahren (siehe \autoref{sec:maths:sor}) verwendet werden. 

\begin{figure}
  \begin{center}
\begin{align*}
    \left(
    \begin{array}{cccc|cccc|cccc|cccc}
    2 & -1 &   &    &-1 & & & & & & & & &    \\
    -1 & 3 & -1 &    & &-1 & & & && & & &    \\
    & -1 & 3 & -1    & & & -1 & & & & & & &    \\
    & & -1 & 2      & & & & -1 & & && & & &    \\
    \hline
    -1& & & &       3& -1 & & &     -1& & & &    \\
    & -1& & &       -1&4& -1 & &     &-1& & & &    \\
    & & -1&         & & -1 & 4 & -1& &&-1& & & &    \\    
    & & &-1         & & & -1 & 3 &  &&&-1& & & &    \\
    \hline
    & & & &    -1& & & &       3& -1 & & &     -1\\
    & & & &    & -1& & &       -1&4& -1 & &     &-1\\
    & & & &    & & -1&         & & -1 & 4 & -1& &&-1\\    
    & & & &    & & &-1         & & & -1 & 3 &  &&&-1\\
    \hline
    & & & &    & & & &    -1& & & &       2& -1\\
    & & & &    & & & &    & -1& & &       -1&3& -1\\
    & & & &    & & & &    & & -1&         & & -1 & 3 & -1\\    
    & & & &    & & & &    & & &-1         & & & -1 & 2\\
    \end{array}
    \right)
\end{align*}
\end{center}
\caption{Schematischer Aufbau der Matrix $R$ für die Berechnung von $\ln E_i$ mit räumlicher Glattheit am Beispiel eines $4 \times 4$ Bildes.}
\label{fig:raum:matrix}
\end{figure}


%------------------------------------------------------------------------------------------------------------------
\section{Erweiterung um Robustheit}
\label{sec:robustheit}

Wie in \autoref{algo:schwachstellen:robustheit} bereits beschrieben, setzt das Verfahren von Debevec und Malik nur quadratische Bestrafungsterme ein. Diese reduzieren die Auswirkungen von Gauß-Rauschen auf den Eingabebildern (künstlich erzeugt oder z.B. durch Unschärfe bei der Aufnahme der Bilder). Bei anderen Messungenauigkeiten (wie z.B. \gls{SaltAndPepperNoise}) ist ein subquadratischer Bestrafungsterm aus Sicht der Robustheit des Verfahrens jedoch besser geeignet, da hier die starken Ausreißer weniger stark bestrafend wirken. Um die Erweiterung um die Robustheit einzuführen, werden die quadratischen Bestrafungsterme an den gewünschten Stellen durch die subquadratischen ersetzt. 


% --------------------------------------------------------------------------------------------------------------
\subsection{Subquadratische Bestrafungsfunktion im Monotonie- oder Glattheits-Term von $\b g$}
An den Termen für die Glattheit von $\b g$ und die Monotonie-Forderung an $\b g$ (siehe \autoref{sec:monotonie}) ergeben die subquadratischen Bestrafungsterme keinen besonderen Sinn, da diese hier zu stückweise linearen Kurven bzw. stückweise monotonen Funktionen führen würden. Aus diesem Grund wurden diese Terme nicht erweitert.


% --------------------------------------------------------------------------------------------------------------
\subsection{Subquadratische Bestrafungsfunktion im Datenterm von $\b g$ und $\b E$}
Der Datenterm von $\b g$ berücksichtigt bisher keine Ausreißer. Sind also starke Ausreißer in den Bildern der Belichtungsserie zu finden (wie z.B. \gls{SaltAndPepperNoise}), dann werden diese den Datenterm quadratisch beeinflussen. Besser wäre es, hier große Ausreißer weniger stark zu gewichten.
Hier kommen die subquadratischen Bestrafungsfunktionen zum Einsatz, die das Energiefunktional aus \autoref{eq:energy:weights} erweitern. Dieses wird dann wieder partiell nach $\b g(k)$ und $\ln E_i$ abgeleitet, um den Optimierungsansatz zu lösen. 

\begin{align}
\label{eq:energy:robust}
\tilde{\Omega} &= 
    \underbrace{\sum_{i=1}^{N} \sum_{j=1}^{P} w^2(Z_{ij})
    \cdot \varphi([\b g(Z_{ij}) - \ln E_i - \ln \Delta t_j]^2)}_{\mbox{Datenterm mit Robustheit }\tilde{\Phi}}
    + \underbrace{\lambda  \sum_{z=Z_{min}+1}^{Z_{max}-1} [w(Z_{ij}) \cdot \b g''(z)]^2}_{\mbox{Glattheitsterm für }g}\\
\frac{\partial \tilde{\Phi}}{\partial \b g(k)} \overset{!}{=} 0 &= 
    2 w^2(k) \sum_{i=1}^{N} \sum_{j=1}^{P}\delta_{Z_{ij}=k} 
    \underbrace{
        \varphi'(
            [\b g(k)-\ln E_i - \ln \Delta t_j]^2
        )
    }_{\mbox{wird festgehalten}}
    (\b g(k)-\ln E_i - \ln \Delta t_j)
\end{align}
\begin{align}
    &\underbrace{
        2w^2(k)\sum_{i=1}^{N} \sum_{j=1}^{P}  
            \delta_{Z_{ij}=k}
            \varphi'(
                [\b g(k)-\ln E_i - \ln \Delta t_j]^2
            )
    }_{\mbox{Matrixeintrag } \tilde{a}_k}\b g(k) \nonumber \\
    &\qquad= 
    \underbrace{
        2w^2(k)\sum_{i=1}^{N} \sum_{j=1}^{P} 
            (\ln E_i +\ln \Delta t_j)
            \varphi'(
                [\b g(k)-\ln E_i - \ln \Delta t_j]^2
            )
            \delta_{Z_{ij}=k}
    }_{\mbox{Vektoreintrag }\tilde{b}_k}
\end{align}

Der dabei vorkommende Bestrafungsfaktor $\varphi'([\b g(k)-\ln E_i - \ln \Delta t_j]^2)$ wird regelmäßig neu berechnet  (aus den alten Werten von $\b g$ und $\ln E_i$) und kann zeitweise festgehalten. Damit fließt dieser nur als Faktor in die Berechnung ein. Die Koeffizienten $\tilde a_k$ und $\tilde b_k$ ersetzen die Matrix- bzw. Vektoreinträge des Gleichungssystemes aus \autoref{eq:matrix:data}. Der Glattheitsterm $\Theta$ bleibt zusammen mit seinen partiellen Ableitungen identisch. Auch mit dieser Erweiterung hat sich die Struktur des LGS für $\b g$ nicht verändert und kann deswegen mit der LU-Zerlegung (siehe \autoref{sec:maths:lu}) gelöst werden. 

\label{subsec:robust:e:daten}
Diese Erweiterung muss nun auch noch bei der Berechnung von $\ln E_i$ berücksichtigt werden. Auch hierzu wird das Energiefunktional $\tilde{\Omega}$ (siehe \autoref{eq:energy:robust}) wieder partiell nach $\ln E_i$ abgeleitet. Aus der \autoref{eq:fi:basic} entsteht dann bei aktiviertem subquadratischem Bestrafungsterm die neue Berechnung von $\ln E_i$:

\begin{align}
    \label{eq:f_i:robust}
    \ln E_i = F_i &= \frac{
        \sum_{j=0}^{P-1} 
            w^2(Z_{ij})
            (\b g(Z_{ij})-\ln \Delta t_j)
            \varphi'(
                [\b g(Z_{ij})-\ln E_i - \ln \Delta t_j]^2
            )
    }
    {
        \sum_{j=0}^{P-1} 
            w^2(Z_{ij})
            \varphi'(
                [\b g(Z_{ij})-\ln E_i - \ln \Delta t_j]^2
            )
    }
\end{align}


Es kann eine schnellere Konvergenz erzielt werden, wenn die einzelnen Berechnungen von $\b g$ bzw. $\ln E_i$ noch häufiger iteriert werden. Um diesen Prozess zu beschleunigen wurden neben den bereits bestehenden Hauptiterationen (siehe \autoref{alg:alternierend:basic}) eine weitere Ebene der Iterationen eingeführt. Auf dieser Ebene wird nur das Lösen nach $\b g$ bzw. $\ln E_i$ wiederholt (siehe \autoref{alg:alternierend:extended}). Dieses Verfahren macht nur Sinn, falls das Monotonie-Kriterium (siehe \autoref{sec:monotonie}) oder die Robustheit mittels subquadratischen Bestrafungstermen aktiviert ist, da hier auf die vorherigen Werte der entsprechenden Unbekannten eingegangen wird. 

Das Abbruchkriterium der inneren Schleife könnte ebenfalls wie beim \gls{SOR} Verfahren (siehe \autoref{sec:maths:sor}) über das relative Residuum erfolgen. In der verwendeten Implementierung wurde jedoch aus Gründen der Einfachheit eine maximale Anzahl an inneren Iterationen festgelegt.

\begin{Algorithmus} %Die Umgebung nur benutzen, wenn man den Algorithmus ähnlich wie Graphiken von TeX platzieren lassen möchte
\caption{Erweitertes alternierendes Lösen nach $g(k)$ und $\ln E_i$ mit Haupt- und Inneniterationen}
\label{alg:alternierend:extended}
\begin{algorithmic}
\Function{SolveHDR}{$Z_{ij}$, $\ln \Delta t_j$, $N$, $P$}
	\State $g \gets initG()$
	\While{$g$ changes} 
		\Repeat
		    \State $F \gets solveF(F, g, Z_{ij}, \ln \Delta t_j, N, P)$ \Comment{$F_i = \ln E_i$}
		\Until{$F$ has not changed significantly} 
		\Repeat
			\State $g \gets solveG(F, g, Z_{ij}, \ln \Delta t_j, N, P)$
		\Until$g$ has not changed significantly
	\EndWhile
	\State \Return [$g$, $F$]
\EndFunction
\end{algorithmic}
\end{Algorithmus}






% --------------------------------------------------------------------------------------------------------------
\subsection{Subquadratische Bestrafungsfunktion im räumlichen Glattheitsterm von $\b E$}
\label{subsec:robust:e:raum}
Die subquadratischen Bestrafungsfunktionen machen auch bei der Betrachtung des räumlichen Glattheitsterms für $\b E$ 
Sinn. Durch die weniger starke Gewichtung von starken Ausreißern (wie sie z.B. typischer Weise an Kanten in Bildern vorkommen) können Strukturen im Bild besser erhalten werden.

Als Grundlage gilt hier der Glattheitsterm $\Psi$ aus der \autoref{eq:raum:glattheit}. Dieser wird nun um die subquadratische Bestrafungsfunktion $\varphi(s^2) = \sqrt{s^2+\epsilon^2}$ erweitert.

\begin{align}
\label{eq:robust:raum}
\tilde{\Psi} =& 
        \alpha \sum_{i\in A}
            \varphi((\ln E_i - \ln E_{i-1})^2)
        +\alpha \sum_{i=n}^{N-1}\varphi((\ln E_i - \ln E_{i-n})^2)
    \\
    A=& \{ i \in [0,N-1]\} \setminus \{ i \cdot k | k \in \mathbb{N} \}
\end{align}

Dies muss nun wieder nach $\ln E_i$ partiell abgeleitet werden. Dabei wurde auch hier $\ln E_i$ durch $F_i$ ersetzt. Zunächst werden die Randbedingungen ebenfalls vernachlässigt.
\begin{align}
\frac{\partial \tilde \Psi}{\partial F_i} =& 2\alpha[ \nonumber\\
    & \qquad + \varphi'((F_i - F_{i-1})^2)     \cdot (F_i - F_{i-1}) \nonumber\\
    & \qquad - \varphi'((F_{i+1} - F_{i})^2)   \cdot (F_{i+1} - F_i) \nonumber\\
    & \qquad + \varphi'((F_i - F_{i-n})^2)     \cdot (F_i - F_{i-n})\nonumber\\
    & \qquad - \varphi'((F_{i+n} - F_{i})^2)   \cdot (F_{i+n}- F_i)\nonumber\\
    ]\\
=& 2\alpha[ \nonumber\\
    & \qquad - \varphi'((F_i - F_{i-1})^2)     \cdot F_{i-1} \nonumber\\
    & \qquad - \varphi'((F_{i+1} - F_{i})^2)   \cdot F_{i+1} \nonumber\\
    & \qquad - \varphi'((F_i - F_{i-n})^2)     \cdot F_{i-n} \nonumber\\
    & \qquad - \varphi'((F_{i+n} - F_{i})^2)   \cdot F_{i+n} \nonumber\\
    & \qquad + \{\varphi'((F_i - F_{i-1})^2) + \varphi'((F_{i+1} - F_{i})^2) +\varphi'((F_i - F_{i-n})^2) + \varphi'((F_{i+n} - F_{i})^2)\} \cdot F_i \nonumber\\
    ] \overset{!}{=} 0
\end{align}

Daraus lässt sich wieder ein Stencil (immer noch ohne Berücksichtigung der Ränder) für den Einfluss der umliegenden Bildpunkte bei der Berechnung von $\ln E_i$ aufstellen (siehe \autoref{fig:robust:raum:stencil}). Die Struktur der Matrix $\tilde R$ (siehe \autoref{fig:raum:matrix}) ist hier wieder ähnlich. An den Rändern fallen entsprechend die Stencil-Einträge an den Seiten weg und treten damit dann auch nicht im zentralen Pixel auf (dieses enthält die positive Summe der Koeffizienten der Umgebungspixel). Auch in dieser Erweiterung wird $\varphi'(s^2)$ vorab berechnet und dann regelmäßig aktualisiert.

\begin{figure}
  \begin{center}
    \begin{tabular}{c|c|c}
        \cline{2-2}
        & $-\varphi'((F_i - F_{i-n})^2)$ & \\
        \hline
            \multicolumn{1}{|c|}{$-\varphi'((F_i - F_{i-1})^2)$}
            & \shortstack{$\varphi'((F_i - F_{i-1})^2) + \varphi'((F_{i+1} - F_{i})^2) $ \\
              $ +\varphi'((F_i - F_{i-n})^2) + \varphi'((F_{i+n} - F_{i})^2)$} & 
            \multicolumn{1}{c|}{$\varphi'((F_{i+1} - F_{i})^2)$}\\
        \hline
        & $-\varphi'((F_{i+n} - F_{i})^2)$ & \\
        \cline{2-2} 
    \end{tabular}
  \end{center}
\caption{Einfluss der umliegenden Bildpunkte $F_i$ bei aktiviertem räumlichen Glattheitsterm mit der Erweiterung durch einen subquadratischen Bestrafungsterm}
\label{fig:robust:raum:stencil}
\end{figure}

Das daraus entstehende Gleichungssystem in Matrix-Schreibweise ähnelt dem aus \autoref{eq:raum:res}. 
\begin{align}
\label{eq:robust:raum:lgs}
2 H \cdot \b F + 2 \alpha \tilde R\cdot \b F =& 2 \b b \\
(H+\alpha \tilde R)\cdot \b F =& \b b
\end{align}






% --------------------------------------------------------------------------------------------------------------
\subsection{Subquadratische Bestrafungsfunktion im Daten- und Glattheitsterm von $\b E$ }
Um bei der Berechnung von $\ln E_i$ sowohl im Datenterm, als auch im Glattheitsterm robuste Bestrafungsfunktionen zu verwenden, müssen die Ergebnisse aus \autoref{subsec:robust:e:daten} und \autoref{subsec:robust:e:raum} kombiniert werden.


\begin{align}
\tilde{\Omega} &= 
    \underbrace{\sum_{i=1}^{N} \sum_{j=1}^{P} w^2(Z_{ij})
    \cdot \varphi([\b g(Z_{ij}) - \ln E_i - \ln \Delta t_j]^2)}_{\mbox{Datenterm mit Robustheit }\tilde{\Phi}}
    + \underbrace{\lambda  \sum_{z=Z_{min}+1}^{Z_{max}-1} [w(Z_{ij}) \cdot \b g''(z)]^2}_{\mbox{Glattheitsterm für }g} \nonumber\\
    & + \underbrace{
        \alpha \sum_{i\in A}
            \varphi((\ln E_i - \ln E_{i-1})^2)
        +\alpha \sum_{i=n}^{N-1}\varphi((\ln E_i - \ln E_{i-n})^2)
    }_{\mbox{räumlicher Glattheitsterm mit Robustheit }\Tilde \Psi}
\\
\end{align}

Die \autoref{eq:f_i:robust} kann ebenfalls so umgeformt werden, dass ein LGS entsteht. 
\begin{align}
    & 2 \underbrace{
        \sum_{j=0}^{P-1} 
            w^2(Z_{ij})
            \varphi'(
                [\b g(Z_{ij})-\ln E_i - \ln \Delta t_j]^2
            )
    }_{\mbox{Matrixeintrag }\tilde h_i}
    \ln E_i = \nonumber \\
    &\qquad 2 \underbrace{
        \sum_{j=0}^{P-1} 
            w^2(Z_{ij})
            (\b g(Z_{ij})-\ln \Delta t_j)
            \varphi'(
                [\b g(Z_{ij})-\ln E_i - \ln \Delta t_j]^2
            )
    }_{\mbox{Vektoreintrag } \tilde b_i}\\
    &
    2 \underbrace{
        \begin{pmatrix}
            \ddots & &\\
            & \tilde h_i & \\
            & & \ddots
        \end{pmatrix}
    }_{\mbox{Matrix }\tilde H}
    \cdot
    \begin{pmatrix}
        \vdots\\ \ln E_i \\ \vdots
    \end{pmatrix}
    =
    2 \underbrace{
        \begin{pmatrix}
            \vdots\\ \tilde b_i \\ \vdots    
        \end{pmatrix}
    }_{\mbox{Vektor }\tilde{\b b}}\\
    & 2 \tilde H \cdot \b F = 2 \tilde{\b{b}}
\end{align}

Dieses Gleichungssystem wird nun mit der partiellen Ableitung von $\tilde \Psi$ kombiniert (siehe \autoref{eq:robust:raum:lgs}), was zum nachfolgenden Gleichungssystem führt.

\begin{align}
2 \tilde H \cdot \b F + 2 \alpha \tilde R\cdot \b F =& 2 \tilde{\b b} \\
(\tilde H+\alpha \tilde R)\cdot \b F =& \tilde{\b b}
\end{align}

Strukturell entspricht dieses LGS dem aus \autoref{sec:raeumlich}, da die Matrix $\tilde H$ eine Diagonalmatrix mit positiven Einträgen ist. Damit bleibt das LGS positiv semidefinit und quadratisch. Auch hier kommt das \gls{SOR} Verfahren zum Einsatz.




% --------------------------------------------------------------------------------------------------------------
\section{Lösung der Gleichungssysteme}
\label{sec:solvers}
Grundsätzlich hat es der Algorithmus mit zwei verschiedenen Matrix-Strukturen zu tun. Bei der Berechnung von $\b g$ wird die strukturelle Eigenschaft der Matrix $M$ ausgenutzt um ein schnelles Lösen mittels der LU-Zerlegung zu gewährleisten.

Bei der Erweiterung des Ansatzes um einen räumlichen Glattheitsterm (siehe \autoref{sec:raeumlich}) tritt außerdem eine positive semidefinite Matrix auf, die gut durch das \gls{SOR} Verfahren gelöst werden kann. Beide Verfahren werden hier kurz vorgestellt.



% --------------------------------------------------------------------------------------------------------------
\subsection{LU-Zerlegung einer Pentadiagonal-Matrix}
\label{sec:maths:lu}
Die Matrix $M$ aus \autoref{eq:g:lgs} ist pentadiagonal. Das bedeutet, es sind nur die zentralen fünf Diagonal-Elemente der Matrix besetzt. Hier kommt eine besonders schnelle Variante der LU-Zerlegung zum Einsatz.

Die LU-Zerlegung ist ein Verfahren, bei dem eine quadratische Matrix $A$ in die beiden Dreiecksmatritzen $L$ und $U$ zerlegt wird. Das besondere an diesem Verfahren ist, dass die nichttrivialen Elemente (Einträge ungleich Null) in der Matrix $L$ (engl. lower) sich nur in der unteren linken bzw. bei der Matrix $U$ (engl. upper) nur in der oberen rechten Hälfte befinden. Die Diagonaleinträge von $L$ haben alle den Wert eins. 

In diesem speziellen Fall ist bekannt, dass die Matrix nur fünf besetzte Diagonalen hat, was zur Struktur der Matritzen in \autoref{eq:lu:structure} führt.

\begin{align}
A &= L \cdot U \\
\label{eq:lu:structure}
\begin{pmatrix}
A_{ij}\\
\end{pmatrix}
&= 
\begin{pmatrix}
1 & 0 &  \\
l_1 & 1 & 0 &\\
k_2 & l_2 & 1 & 0 &\\
0 & k_3 & l_3 & 1 & 0 &\\
  &   \ddots  & \ddots & \ddots & \ddots\\
  & & 0& k_n & l_n & 1\\
\end{pmatrix}
\cdot
\begin{pmatrix}
m_0 & r_0 & p_0 & 0 &  \\
0 & m_1 & r_1 & p_1 & 0 &  \\
 & \ddots& \ddots & \ddots & \ddots & 0\\
 & & \ddots&\ddots&\ddots & p_{n-2}\\
 & & &\ddots&\ddots & r_{n-1}\\
&&  & &0& m_n 
\end{pmatrix}
\end{align}

Daraus lässt sich dann der \autoref{alg:LU} herleiten, der eine pentadiagonale Matrix $A$ und einen Vektor $\b b$ als Eingabe hat und den Vektor $\b x$ zurückgibt, sodass gilt $A \cdot \b x = \b b$. Das darin enthaltende Lösen des LGS $A\cdot \b x = \b b$ geschieht mittels der Vorwärts-Eliminierung und der Rückwärts-Substitution. 

Eine erweiterte Pivotisierung der Spalten ist nicht notwendig, da die Diagonal-Einträge der Matrix bereits die betragsmäßig größten Werte einer Spalte haben. Der komplette Algorithmus ist in Pseudocode im Anhang zu finden (siehe \ref{alg:LU}).



% --------------------------------------------------------------------------------------------------------------
\subsection{SOR-Algorithmus}
\label{sec:maths:sor}

Für die Erweiterung um einen räumlichen Glattheitsterm (siehe \autoref{sec:robustheit}) musste außerdem noch ein weiterer Typ Matrix gelöst werden. Das dabei entstehende Gleichungssystem hätte nicht so effizient mit der LU-Zerlegung gelöst werden können, da die dabei entstehende Matrix nicht pentadiagonal ist und außerdem sehr groß ist ($N \times N$, wobei $N$ die Anzahl der Pixel in einem Bild der Belichtungsserie ist). 

Hierfür scheint das \gls{SOR} Verfahren besser geeignet zu sein. Bei Matritzen, die quadratisch, positiv definit, symmetrischen und dünn besetzt sind, stellt das \gls{SOR} eine Verbesserung gegenüber dem Gauß-Seidel Verfahren dar. Der reelle Parameter $\omega \in (0,2)$ sorgt dafür, dass das Verfahren schneller konvergiert. Das Gauß-Seidel und das \gls{SOR} Verfahren sind identisch für $\omega = 1 $.

Die Lösung für $x$ wird komponentenweise und iterativ nach folgender Vorschrift bestimmt:
\begin{align}
x_k^{m+1} = (1-\omega)x_k^m+ \frac{\omega}{a_{kk}}(b_k - \sum_{i>k} a_{ki}x_i^m - \sum_{i<k} a_{ki}x_{i}^{m+1}) ,\; k \in [1, n]
\end{align}

Die Implementierung des Verfahrens verwendet als Abbruchkriterium die maximale Komponente $r_{max}$ des Residuum-Vektors $\b r$ mit $\b r = A \cdot \b x - \b b$. Diese wird nach jeder Iteration $m$ mit $r_{max}^m := \max_{i\in [1,n]} |r_i^m| < \delta_1$ berechnet. Falls sowohl das Residuum  als auch die Differenz der Werte zweier aufeinander folgender Iterationen kleiner als die vorgegebenen Schranken sind, so terminiert das Verfahren \cite[S. 143]{Westermann2008}. In dieser Implementierung wurde außerdem eine maximale obere Schranke für die Anzahl der Iterationen angegeben.


